<div class="orbitron">Implementation Considerations</div>
<div class="m24">
        <p>
                What normally undergoes curation for implementation are external datasets. These are simple to evaluate because the data is usually handled via authentication through API. As long as the API is not rate-limited with extreme restrictions like the one's provided by commercial entities, then it is usually capable of being implemented. A case for consideration is the repurposing of established systems. If any system contains open-data that is as abundant as Mediawiki, then it deserves to be developed for.
        </p>
        <p>
                Mediawiki is an implementation which was possible because the base application is php. Mediawiki at the library does not run as a stand-alone application like it usually does on the web. It is instead used as a php-library which manages data and serves raw wikitext that is parsed manually with a set of php scripts. This allows all Mediawiki datasets to use the same css styles of the library while also making unruly elements such as tables (with too many columns) become readable in a narrow viewing space, by turning the tables into carded data. A Mediawiki implementation serves to improve readability and access in multiple ways while minimizing sacrifices. For example, since the <em>table of contents</em> is newly introduced user-interface element that takes up horizontal space, removed are the <em>infoboxes</em> in the top right side of the page that are commonly mainstays in wikipedia layouts. These elements are instead simply rendered on top of the article as if the page were always in mobile view.
        </p>
        <p>For Mediawiki, wikitext parsing is not an easy task however. Every Mediawiki dataset so far has had different tables and templates, which requires more parsing rules to always be added. Out of a multi-thousand page dataset, it sometimes takes a while to evaluate pages for parsing errors, especially if the dataset has had a history of inconsistent page construction such as the Hot Wheels Fandom page. </p>
</div>
<div class="orbitron">Article Base</div>
<div class="m24">
        <p>The library is not limited to the presentation of external datasets. It has the Music Theory application and its own dictionary that is always available at the bottom left side of the screen. Knowing this, it is possible to create more in-house applications to fill the gaps of the external datasets.
                An article base would be a new application type which serves human written &amp; composed articles of any topics under-represented by other implementations. Navigation starts from an organized index page, similar to how other datasets generally present themselves. The bespoke nature of a human written article allows for the shaping of information that is beyond what ordered records are able to accomplish on their own. Such content would fill in cultural and historical gaps, topics that are absent from all accumulated datasets. 
        </p>
        <p>
                An implementation such as an article base would be green-lit when progress for institutional API's have settled. It would require a content management system on the front-end with a new database on server-side. Data structures are up in the air as of yet, which is an exciting part of development. It can be anything, collections of static html, ordered JSON data that is parsed on page load, or even be its own Mediawiki dataset where pages are composed in the Mediawiki editor. 
        </p>
</div>