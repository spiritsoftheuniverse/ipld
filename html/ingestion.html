<div class="orbitron">Ingestion</div>
<div class="m24">
        <p>The general flow of dataset ingestion from an API to local disk:</p>
                <ul>
                        <li>Run a php script which iterates through facets of an API and saves each record as an individual JSON file.</li>
                        <li>Create the database and its schematics based on curatorial analysis for the structure of a record's JSON data.</li>
                        <li>Run a php script which inserts each record from the JSON files into the database.</li>
                        <li>Export filter/category data into a JSON cache file for the front-end to read on page-load and parse as html.</li>
                </ul>
        <p>
                Mediawiki datasets use the following process for ingestion:
                <ul>
                        <li>A data dump is requested directly from Fandom if one is too old or has never been created.</li>
                        <li>The data dump is inserted into the database using the designated Mediawiki maintenance script.</li>
                </ul>
        </p>
</div>
<div class="orbitron">Ingestion Failure</div>
<div class="m24">
        Even if a dataset is cleared for implementation during the curation process, the ingestion process for said dataset may be too taxing of a procedure. An example of a dataset likely to fail during ingestion would be a Mediawiki dataset which relies on too many Mediawiki extensions, or uses extensions which conflict with the custom-parser which determines the final html output of page data.
</div>
<div class="orbitron">Synchronization</div>
<div class="m24">
        <p>
                The library has an administrative synchronization page in the front-end which polls a log file for progress while a few buttons dictate asynchronous commands to the command line interface.
                There are currently no automation tools for synchronization such as cron jobs since every sync job as of yet requires monitoring for edge case errors.
        </p>
        <p>A php synchronization script for datasets from APIs is ideally a refactored version of the ingestion script which automatically inserts new records into the database or updates them directly instead of saving them as JSON files. </p>
        <p>
                Some synchronization involves clearing an entire database and reinserting data from a freshly downloaded dump. Implemented datasets which synchronize using this method include Rebrickable, PokemonTCG, and Scryfall, as they regularly dump their databases.
        </p>
        <p>Synchronization for Mediawiki datasets are developed as maintenance script extensions which run with the Mediawiki php library:</p>
                <ul>
                        <li>The fetching script downloads all pages created or updated from within range of a specified date and saves them all as individual JSON files.</li>
                        <li>The create/update script reads each JSON file and creates non-existing pages from them or updates any pages where the local revision is older.</li>
                </ul>
</div>