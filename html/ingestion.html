<div class="orbitron">Ingestion</div>
<div class="m24">
        <p>The general flow of dataset ingestion from the server to local disk is:</p>
                <ul>
                        <li>Run a php script which iterates through facets of an API and saves each record as an individual JSON file.</li>
                        <li>Create the database and its schematics based on curatorial analysis for the structure of a record's JSON data.</li>
                        <li>Run a php script which inserts each record from the JSON files into the database.</li>
                        <li>Export filter/category data into a JSON file for the front-end to read on page-load.</li>
                </ul>
        <p>
                Mediawiki datasets use the following process for ingestion:
                <ul>
                        <li>A data dump is requested directly from Fandom if one is too old or has never been created.</li>
                        <li>The data dump is inserted into the database using the designated Mediawiki maintenance script.</li>
                </ul>
        </p>
</div>
<div class="orbitron">Synchronization</div>
<div class="m24">
        <p>The php synchronization script for datasets from APIs is ideally a refactored version of the ingestion script which automatically inserts new records into the database or updates them directly instead of saving them as JSON files. </p>
        <p>Synchronization for Mediawiki datasets are developed as maintenance script extensions which run with the Mediawiki php library:</p>
                <ul>
                        <li>The fetching script downloads all pages created or updated from within range of a specified date and saves them all as individual JSON files.</li>
                        <li>The create/update script reads each JSON file and creates non-existing pages from them or updates any pages where the local revision is older.</li>
                </ul>
</div>