<div class="orbitron">Ingestion</div>
<div class="m24">
        <p>
                Ingestion of datasets is a regular job. At times, half of the threads from the CPU are running ingestion scripts in concurrency. Some datasets takes minutes, hours, or days to ingest. Few take weeks or months. Ingestion scripts are written with respect for all rate-limits. 
                In the case of missing data from earlier, less experienced ingestions, a new script is written in order to map the full data, and the ingestion is restarted from the first record.
        </p>
        <p>The general flow of dataset ingestion from an API to local disk:</p>
                <ul>
                        <li>Run a php script which iterates through facets of an API and saves each record as an individual JSON file.</li>
                        <li>Create the database and its schematics based on curatorial analysis of a record's JSON data structure.</li>
                        <li>Run a php script which inserts each record from the JSON files into the database.</li>
                        <li>Export filter/category data into a JSON cache file for the front-end to read on page-load and parse as html.</li>
                </ul>
        <p> Mediawiki datasets use the following process for ingestion:</p>
                <ul>
                        <li>A data dump is requested directly from Fandom if one is too old or has never been created.</li>
                        <li>The data dump is inserted into the database using the designated Mediawiki maintenance script.</li>
                </ul>
        
</div>
<div class="orbitron">Ingestion Failure</div>
<div class="m24">
        Even if a dataset is cleared for implementation, the ingestion process may end up taxing. An example of a dataset likely to fail during ingestion would be a Mediawiki dataset that relies on too many Mediawiki extensions, which leads to conflict with the custom-parser that determines the final html output.
</div>
<div class="orbitron">Synchronization</div>
<div class="m24">
        <p>
                The library has an administrative synchronization page in the front-end which polls a log file for progress while the CLI is running a job. 
                Every implementation that is capable of data-synchronization is listed as a toggle button that is used to determine what CLI script is to be ran.
                There are currently no automation tools for synchronization such as cron jobs since some sync jobs still require being present for debugging.
        </p>
        <p>A php synchronization script for datasets from APIs is ideally a rewritten version of the ingestion script that automatically inserts new records into the database or updates them directly instead of saving them as JSON files. </p>
        <p>
                Some synchronization involves clearing an entire database and reinserting data from a freshly downloaded dump. Implemented datasets which synchronize using this method include Rebrickable, PokemonTCG, and Scryfall, as they regularly dump their databases.
        </p>
        <p>Synchronization for Mediawiki datasets are developed as maintenance script extensions which run with the Mediawiki php library:</p>
        <ul>
                <li>The fetching script downloads all pages created or updated from within range of a specified date and saves them all as individual JSON files.</li>
                <li>The create/update script reads each JSON file and creates non-existing pages from them or updates any pages where the local revision is older.</li>
        </ul>
</div>